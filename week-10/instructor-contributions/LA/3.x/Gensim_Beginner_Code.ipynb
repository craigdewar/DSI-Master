{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim import corpora\n",
    "from gensim import matutils\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from gensim import corpora, models, similarities\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'\\ufeffMr. Chairman, delegates. I accept your nomination for President of the United States of America.\\r\\n\\r\\nI do so with humility, deeply moved by the trust you have placed in me. It is a great honor. It is an even greater responsibility.\\r\\n\\r\\nTonight I am asking you to join me to walk together to a better future. By my side, I have chosen a man with a big heart from a small town. He re'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_path = os.path.expanduser('~') + \"romney_2012_rnc.txt\"\n",
    "data_path = \"romney_2012_rnc.txt\"\n",
    "with open(data_path, 'rw+') as f:\n",
    "    data = f.read().strip().decode('utf-8')\n",
    "\n",
    "data[:380]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stops = set(stopwords.words(\"english\")) \n",
    "\n",
    "def clean_tokenize(words):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    \n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(words, \"lxml\").get_text() \n",
    "    \n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    \n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    \n",
    "    # 4. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    \n",
    "    # 5. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return tokenize.word_tokenize(( \" \".join( meaningful_words )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = clean_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_top_topic(docs):\n",
    "    # Function to create a list of integers which represent the top topic for each person. \n",
    "    # I could have looked at the top 5 topics too and build a longer topic vector to compare 2 users but I keep it simple for now\n",
    "    \n",
    "    # create a list of cleaned and tokenized documents\n",
    "    parsed = [clean_tokenize(s) for s in docs]\n",
    "    \n",
    "    # create a dictionary for each unique word from all documents\n",
    "    dictionary = corpora.Dictionary(parsed)\n",
    "    print dictionary\n",
    "    # ignore words that appear in less than 20 documents or more than 10% documents\n",
    "    dictionary.filter_extremes(no_below=20, no_above=0.1)\n",
    "    print dictionary\n",
    "    \n",
    "    # create a list of bow for each document (worddicID, count)\n",
    "    corpus = [dictionary.doc2bow(text) for text in parsed] # bow for each row in documents\n",
    "    \n",
    "    # what is the most common word in the that article?\n",
    "    most_index, most_count = max(corpus[0], key=lambda (word_index, count): count)\n",
    "    print most_index, most_count\n",
    "    print dictionary[most_index]\n",
    "    \n",
    "    # create a tfidf from bow\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus] # tfidf for each row in documents\n",
    "    \n",
    "    lda = LdaModel(corpus_tfidf, id2word=dictionary, num_topics=20, update_every=0, passes=20)\n",
    "    lda.print_topics(num_topics=20, num_words=20)\n",
    "    \n",
    "    get_top_topic = lambda person: sorted(lda.get_document_topics(person), key= lambda (x,y): y, reverse=True)[0][0]\n",
    "    \n",
    "    return [get_top_topic(person) for person in corpus_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
