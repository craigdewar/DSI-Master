{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load data\n",
    "\n",
    "I'll be using a voting dataset I made.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV, ElasticNetCV\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "issues = pd.read_csv('./datasets/city_issues_nlp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create X, Y, and normalize X\n",
    "\n",
    "**X is num_votes here**. We're doing regression.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Linear regression\n",
    "\n",
    "1. Cross-validate a linear regression with 5 folds. \n",
    "2. Build a linear regression model with the full X and Y.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Ridge regression\n",
    "\n",
    "1. Either use `GridSearchCV` or `RidgeCV` to find the best `C` or `alpha` respectively. **Remember that bigger alphas means stronger regularization, and smaller Cs are stronger regularization. (C is the inverse of alpha).**\n",
    "2. Cross-validate the R2 with Ridge using your optimal C or alpha.\n",
    "3. Build a final Ridge model and fit it on the full X and Y as you did above.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Lasso regression\n",
    "\n",
    "1. Use either `GridSearchCV` or `LassoCV` to find the optimal `C` or `alpha` for the Lasso regression.\n",
    "2. Cross-validate the R2 with Lasso using your optimal C or alpha.\n",
    "3. Build a final Lasso model fit on the full X and Y.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. ElasticNet regression\n",
    "\n",
    "Now you'll get to try out the ElasticNet. It is a combination of the Ridge and the Lasso to leverage the benefits of both!\n",
    "\n",
    "Arguments to optimize:\n",
    "\n",
    "    alpha : same as the Ridge/Lasso above\n",
    "    l1_ratio: this is the proportion of Ridge vs Lasso that the model is. \n",
    "        An l1_ratio of 0.0 is a pure Ridge\n",
    "        An l1_ratio of 1.0 is a pure Lasso\n",
    "        \n",
    "1. Use `GridSearchCV` or `ElasticNetCV` to search for the optimal `alpha` and `l1_ratio`. \n",
    "2. Explain the probable reason why the it chose the parameters it did as the best ones.\n",
    "3. Cross-validate the R2 with the ElasticNet using the optimal parameters.\n",
    "4. Fit the ElasticNet on all X and Y.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. DecisionTreeRegressor\n",
    "\n",
    "1. Use `GridSearchCV` to find the best `max_features`, `max_depth`, and `min_samples_leaf`. Read the documentation and think about what range of parameters would be good to search for each!\n",
    "2. Cross-validate the R2 as above.\n",
    "3. Fit a DecisionTreeRegressor on all X and Y as above.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. BaggingRegressor\n",
    "\n",
    "Now we'll use bagging with the DecisionTreeRegressor. Yes, regressions can be done with bagging too!\n",
    "\n",
    "---\n",
    "\n",
    "Remember that with the Bagging regressor you first have to initialize the internal \"base estimator\" that it will copy:\n",
    "\n",
    "```python\n",
    "dtr = DecisionTreeRegressor()\n",
    "```\n",
    "\n",
    "A cool thing to note is that you can actually gridseach over the internal base estimators as well. So, not only are you finding the best parameters for the BaggingRegressor but also the DecisionTreeRegressors that it copies inside:\n",
    "\n",
    "```python\n",
    "bag_params = {\n",
    "    'base_estimator__max_features':[None],\n",
    "    'base_estimator__max_depth':[None],\n",
    "    'base_estimator__min_samples_leaf':[1],\n",
    "    'max_features': [0.33, 0.66, 0.99],\n",
    "    'max_samples': [0.1, 0.2, 0.4, 0.6, 0.8, 0.9],\n",
    "    'n_estimators': [100]\n",
    "}\n",
    "```\n",
    "\n",
    "**Be careful putting too many parameters into the `GridSearchCV`! It can really explode the possible permutations!**\n",
    "\n",
    "That being said, you'll probably be able to put a decent amount of parameters in since the wine dataset doesn't have many columns.\n",
    "\n",
    "Next you initialize the BaggingRegressor, putting the desired model as the first argument:\n",
    "\n",
    "```python\n",
    "bag = BaggingRegressor(dtr)\n",
    "```\n",
    "\n",
    "This tells the BaggingRegressor that you want it to spawn DecisionTreeRegressors as it's internal \"children\" base estimators.\n",
    "\n",
    "Lastly, you'll put the BaggingClassifier into the grid searcher and fit on the data (it will cross-validate with the specified `cv` folds.\n",
    "\n",
    "```python\n",
    "bag_gs = GridSearchCV(bag, bag_params, cv=5, verbose=1)\n",
    "bag_gs.fit(X, Y)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "As before...\n",
    "\n",
    "1. Use `GridSearchCV` to find the best `BaggingRegressor` and optionally internal `DecisionTreeRegressor` parameters.\n",
    "2. Cross-validate the R2.\n",
    "3. Fit a `BaggingRegressor` on all X and Y with the optimal parameters.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Get feature importances from `RandomForestRegressor`\n",
    "\n",
    "The `RandomForestRegressor` has an attribute called `.feature_importances_`. These are the importances of your predictors as measured by how useful they were to the base estimators.\n",
    "\n",
    "As you may recall, the `RandomForestRegressor` is just a special case of the `BaggingRegressor` that specifically uses decision trees. In fact, you've already done it above. The difference is that this class gives us the feature importances whereas the \"generalized\" bagging regressor class does not.\n",
    "\n",
    "1. Save the column names X in a variable.\n",
    "2. Grid search optimal parameters for the `RandomForestRegressor`.\n",
    "3. Fit a `RandomForestRegressor` using the optimal parameters you found on the full X and Y.\n",
    "4. Get out the feature importances.\n",
    "5. Create a pandas DataFrame where one column is the feature importances and the other column is the X column names.\n",
    "6. Sort the dataframe you made by feature importances in descending order.\n",
    "7. Plot the feature importances.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "f, ax = plt.subplots(figsize=(6, 15))\n",
    "\n",
    "sns.set_color_codes(\"muted\")\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=feature_importances,\n",
    "            label=\"feature importances\", color=\"b\")\n",
    "\n",
    "sns.despine(left=True, bottom=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. [BONUS] Use a different regression class with the `BaggingRegressor`\n",
    "\n",
    "You could try `Ridge`, `Lasso`, `ElasticNet`, `SVC`, `KNeighborsRegressor` or any kind of regression class you're interested in!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
