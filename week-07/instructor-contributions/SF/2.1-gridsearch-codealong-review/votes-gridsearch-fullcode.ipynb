{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load data\n",
    "\n",
    "I'll be using a voting dataset I made.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV, ElasticNetCV\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "issues = pd.read_csv('./datasets/city_issues_nlp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create X, Y, and normalize X\n",
    "\n",
    "**X is num_votes here**. We're doing regression.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = issues.num_votes.values\n",
    "\n",
    "X = issues[[x for x in issues.columns if x not in ['num_votes']]]\n",
    "X_cols = X.columns\n",
    "X = (X - X.mean())/X.std()\n",
    "X = X.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Linear regression\n",
    "\n",
    "1. Cross-validate a linear regression with 5 folds. \n",
    "2. Build a linear regression model with the full X and Y.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.74117855525e+27\n"
     ]
    }
   ],
   "source": [
    "linreg = LinearRegression()\n",
    "linreg_score = np.mean(cross_val_score(linreg, X, Y, cv=5))\n",
    "linreg.fit(X,Y)\n",
    "print linreg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Ridge regression\n",
    "\n",
    "1. Either use `GridSearchCV` or `RidgeCV` to find the best `C` or `alpha` respectively. **Remember that bigger alphas means stronger regularization, and smaller Cs are stronger regularization. (C is the inverse of alpha).**\n",
    "2. Cross-validate the R2 with Ridge using your optimal C or alpha.\n",
    "3. Build a final Ridge model and fit it on the full X and Y as you did above.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed:    9.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.276137927598\n"
     ]
    }
   ],
   "source": [
    "ridge_params = {\n",
    "    'alpha':np.logspace(0,4,50)\n",
    "}\n",
    "\n",
    "ridge_gs = GridSearchCV(Ridge(), ridge_params, cv=5, verbose=1, n_jobs=-1)\n",
    "ridge_gs.fit(X, Y)\n",
    "\n",
    "ridge = ridge_gs.best_estimator_\n",
    "ridge_score = np.mean(cross_val_score(ridge, X, Y, cv=5))\n",
    "ridge.fit(X,Y)\n",
    "print ridge_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Lasso regression\n",
    "\n",
    "1. Use either `GridSearchCV` or `LassoCV` to find the optimal `C` or `alpha` for the Lasso regression.\n",
    "2. Cross-validate the R2 with Lasso using your optimal C or alpha.\n",
    "3. Build a final Lasso model fit on the full X and Y.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    9.0s\n",
      "[Parallel(n_jobs=-1)]: Done 646 tasks      | elapsed:   19.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:   26.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.316287188052\n"
     ]
    }
   ],
   "source": [
    "lasso_params = {\n",
    "    'alpha':np.logspace(-2,2,200)\n",
    "}\n",
    "\n",
    "lasso_gs = GridSearchCV(Lasso(), lasso_params, cv=5, verbose=1, n_jobs=-1)\n",
    "lasso_gs.fit(X, Y)\n",
    "\n",
    "lasso = lasso_gs.best_estimator_\n",
    "lasso_score = np.mean(cross_val_score(lasso, X, Y, cv=5))\n",
    "lasso.fit(X,Y)\n",
    "print lasso_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. ElasticNet regression\n",
    "\n",
    "Now you'll get to try out the ElasticNet. It is a combination of the Ridge and the Lasso to leverage the benefits of both!\n",
    "\n",
    "Arguments to optimize:\n",
    "\n",
    "    alpha : same as the Ridge/Lasso above\n",
    "    l1_ratio: this is the proportion of Ridge vs Lasso that the model is. \n",
    "        An l1_ratio of 0.0 is a pure Ridge\n",
    "        An l1_ratio of 1.0 is a pure Lasso\n",
    "        \n",
    "1. Use `GridSearchCV` or `ElasticNetCV` to search for the optimal `alpha` and `l1_ratio`. \n",
    "2. Explain the probable reason why the it chose the parameters it did as the best ones.\n",
    "3. Cross-validate the R2 with the ElasticNet using the optimal parameters.\n",
    "4. Fit the ElasticNet on all X and Y.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.316278012198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/coordinate_descent.py:466: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "enet_cv = ElasticNetCV(l1_ratio=np.linspace(0.01,1.0,25),\n",
    "                       n_alphas=100, cv=5, n_jobs=-1)\n",
    "enet_cv.fit(X,Y)\n",
    "\n",
    "enet = ElasticNet(l1_ratio=enet_cv.l1_ratio_, alpha=enet_cv.alpha_)\n",
    "enet_score = np.mean(cross_val_score(enet, X, Y, cv=5))\n",
    "enet.fit(X,Y)\n",
    "print enet_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. DecisionTreeRegressor\n",
    "\n",
    "1. Use `GridSearchCV` to find the best `max_features`, `max_depth`, and `min_samples_leaf`. Read the documentation and think about what range of parameters would be good to search for each!\n",
    "2. Cross-validate the R2 as above.\n",
    "3. Fit a DecisionTreeRegressor on all X and Y as above.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.350793681653\n"
     ]
    }
   ],
   "source": [
    "dtr = DecisionTreeRegressor()\n",
    "\n",
    "dtr_params = {\n",
    "    'max_features':[None],\n",
    "    'max_depth':[1,2,3,None],\n",
    "    'min_samples_leaf':np.linspace(1,101,5)\n",
    "}\n",
    "\n",
    "dtr_gs = GridSearchCV(dtr, dtr_params, cv=5)\n",
    "dtr_gs.fit(X,Y)\n",
    "\n",
    "dtr = dtr_gs.best_estimator_\n",
    "dtr_score = np.mean(cross_val_score(dtr, X, Y, cv=5))\n",
    "dtr.fit(X,Y)\n",
    "print dtr_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. BaggingRegressor\n",
    "\n",
    "Now we'll use bagging with the DecisionTreeRegressor. Yes, regressions can be done with bagging too!\n",
    "\n",
    "---\n",
    "\n",
    "Remember that with the Bagging regressor you first have to initialize the internal \"base estimator\" that it will copy:\n",
    "\n",
    "```python\n",
    "dtr = DecisionTreeRegressor()\n",
    "```\n",
    "\n",
    "A cool thing to note is that you can actually gridseach over the internal base estimators as well. So, not only are you finding the best parameters for the BaggingRegressor but also the DecisionTreeRegressors that it copies inside:\n",
    "\n",
    "```python\n",
    "bag_params = {\n",
    "    'base_estimator__max_features':[None],\n",
    "    'base_estimator__max_depth':[None],\n",
    "    'base_estimator__min_samples_leaf':[1],\n",
    "    'max_features': [0.33, 0.66, 0.99],\n",
    "    'max_samples': [0.1, 0.2, 0.4, 0.6, 0.8, 0.9],\n",
    "    'n_estimators': [100]\n",
    "}\n",
    "```\n",
    "\n",
    "**Be careful putting too many parameters into the `GridSearchCV`! It can really explode the possible permutations!**\n",
    "\n",
    "That being said, you'll probably be able to put a decent amount of parameters in since the wine dataset doesn't have many columns.\n",
    "\n",
    "Next you initialize the BaggingRegressor, putting the desired model as the first argument:\n",
    "\n",
    "```python\n",
    "bag = BaggingRegressor(dtr)\n",
    "```\n",
    "\n",
    "This tells the BaggingRegressor that you want it to spawn DecisionTreeRegressors as it's internal \"children\" base estimators.\n",
    "\n",
    "Lastly, you'll put the BaggingClassifier into the grid searcher and fit on the data (it will cross-validate with the specified `cv` folds.\n",
    "\n",
    "```python\n",
    "bag_gs = GridSearchCV(bag, bag_params, cv=5, verbose=1)\n",
    "bag_gs.fit(X, Y)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "As before...\n",
    "\n",
    "1. Use `GridSearchCV` to find the best `BaggingRegressor` and optionally internal `DecisionTreeRegressor` parameters.\n",
    "2. Cross-validate the R2.\n",
    "3. Fit a `BaggingRegressor` on all X and Y with the optimal parameters.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   34.6s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.406788148278\n"
     ]
    }
   ],
   "source": [
    "dtr = DecisionTreeRegressor()\n",
    "\n",
    "bag_params = {\n",
    "    'base_estimator__max_features':[None],\n",
    "    'base_estimator__max_depth':[None],\n",
    "    'base_estimator__min_samples_leaf':[1],\n",
    "    'max_features': [0.33, 0.66, 0.99],\n",
    "    'max_samples': [0.1, 0.2, 0.4, 0.6, 0.8, 0.9],\n",
    "    'n_estimators': [100]\n",
    "}\n",
    "\n",
    "bag = BaggingRegressor(dtr)\n",
    "\n",
    "bag_gs = GridSearchCV(bag, bag_params, cv=5, verbose=1, n_jobs=-1)\n",
    "bag_gs.fit(X,Y)\n",
    "\n",
    "bag = bag_gs.best_estimator_\n",
    "bag_score = np.mean(cross_val_score(bag, X, Y, cv=5))\n",
    "bag.fit(X,Y)\n",
    "print bag_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Get feature importances from `RandomForestRegressor`\n",
    "\n",
    "The `RandomForestRegressor` has an attribute called `.feature_importances_`. These are the importances of your predictors as measured by how useful they were to the base estimators.\n",
    "\n",
    "As you may recall, the `RandomForestRegressor` is just a special case of the `BaggingRegressor` that specifically uses decision trees. In fact, you've already done it above. The difference is that this class gives us the feature importances whereas the \"generalized\" bagging regressor class does not.\n",
    "\n",
    "1. Save the column names X in a variable.\n",
    "2. Grid search optimal parameters for the `RandomForestRegressor`.\n",
    "3. Fit a `RandomForestRegressor` using the optimal parameters you found on the full X and Y.\n",
    "4. Get out the feature importances.\n",
    "5. Create a pandas DataFrame where one column is the feature importances and the other column is the X column names.\n",
    "6. Sort the dataframe you made by feature importances in descending order.\n",
    "7. Plot the feature importances.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   55.6s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-8d565fadba58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mrf_gs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mrf_gs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_gs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m         \"\"\"\n\u001b[0;32m--> 804\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[1;32m    551\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m                                     error_score=self.error_score)\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 for train, test in cv)\n\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    810\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    760\u001b[0m                         \u001b[0;31m# a working pool as they expect.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rf_params = {\n",
    "    'max_features':['auto',None],\n",
    "    'max_depth':[1,2,3,None],\n",
    "    'min_samples_leaf':np.linspace(1,101,5),\n",
    "    'n_estimators':[250]\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "rf_gs = GridSearchCV(rf, rf_params, cv=5, verbose=1, n_jobs=-1)\n",
    "rf_gs.fit(X, Y)\n",
    "\n",
    "rf = rf_gs.best_estimator_\n",
    "rf_score = np.mean(cross_val_score(rf, X, Y, cv=5))\n",
    "rf.fit(X, Y)\n",
    "print rf_score\n",
    "\n",
    "feat_imp = rf.feature_importances_\n",
    "feature_importances = pd.DataFrame({'feature':X_cols, 'importance':feat_imp})\n",
    "feature_importances.sort_values(['importance'], ascending=False)\n",
    "\n",
    "print feature_importances.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "f, ax = plt.subplots(figsize=(6, 15))\n",
    "\n",
    "sns.set_color_codes(\"muted\")\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=feature_importances,\n",
    "            label=\"feature importances\", color=\"b\")\n",
    "\n",
    "sns.despine(left=True, bottom=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. [BONUS] Use a different regression class with the `BaggingRegressor`\n",
    "\n",
    "You could try `Ridge`, `Lasso`, `ElasticNet`, `SVC`, `KNeighborsRegressor` or any kind of regression class you're interested in!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
