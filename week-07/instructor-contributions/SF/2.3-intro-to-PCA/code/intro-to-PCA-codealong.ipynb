{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Principal Components Analysis (PCA)\n",
    "\n",
    "---\n",
    "\n",
    "PCA is a very popular technique for performing \"dimensionality reduction\" on your data. \n",
    "\n",
    "Dimensionality reduction is the process of combining or collapsing your existing features (columns in X) into new features that not only retain the original information but also ideally reduce noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is PCA?\n",
    "\n",
    "---\n",
    "\n",
    "Technically speaking, PCA finds the linear combinations of your current predictor variables that will create new \"principal components\" that explain, in order, the maximum possible amount of variance in your predictors.\n",
    "\n",
    "That's likely confusing, and will hopefully become clearer soon. \n",
    "\n",
    "The more intuitive way of thinking about PCA is that **it transforms the coordinate system so that the axes become the  most concise, informative descriptors of our data as a whole.**\n",
    "\n",
    "The new axes are the principal components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Process of PCA \n",
    "\n",
    "---\n",
    "\n",
    "Say we have a matrix $X$ of predictor variables. PCA will give us the ability to transform our $X$ matrix into a new matrix $Z$. \n",
    "\n",
    "First we will derive a **weighting matrix** $W$ from the correlational/covariance structure of $X$ that allows us to perform the transformation.\n",
    "\n",
    "Each successive dimension (column) in $Z$ will be rank-ordered according to variance in it's values!\n",
    "\n",
    "**There are 3 assumptions that PCA makes:**\n",
    "1. Linearity: Our data does not hold nonlinear relationships.\n",
    "2. Large variances define importance: our dimensions are constructed to maximize remaining variance.\n",
    "3. Principal components are orthogonal: each component (columns of $Z$) is completely un-correlated with the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Eigenvalues and Eigenvectors\n",
    "\n",
    "---\n",
    "\n",
    "(The jargon in PCA is really the scariest part. Once you get a feel for what these concepts are it's not that bad!)\n",
    "\n",
    "[The next few graphics are shamelessly stolen from this blog...](https://georgemdallas.wordpress.com/2013/10/30/principal-component-analysis-4-dummies-eigenvectors-eigenvalues-and-dimension-reduction/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Image(filename='../assets/images/eigenvalue.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Image(filename='../assets/images/eigenvectors_orthogonal.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Image(filename='../assets/images/transformed_xy.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**EIGENVECTORS**\n",
    "\n",
    "An eigenvector specifies a direction through the original coordinate space. The eigenvector with the highest correspoding eigenvalue is the first principal component.\n",
    "\n",
    "---\n",
    "\n",
    "**EIGENVALUES**\n",
    "\n",
    "Eigenvalues indicate the amount of variance in the direction of it's corresponding eigenvector.\n",
    "\n",
    "---\n",
    "\n",
    "**Every eigenvector has a corresponding eigenvalue!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Image(filename='../assets/images/pca_coordinate_transformation.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### \"Principal Components\"\n",
    "\n",
    "---\n",
    "\n",
    "What is a principal component? **Principal components are the vectors that define the new coordinate system for your data.** Transforming your original data columns onto the principal component axes constructs new variables that are optimized to explain as much variance as possible and to be independent (uncorrelated).\n",
    "\n",
    "Creating these variables is a well-defined mathematical process, but in essence **each component is created as a weighted sum of your original columns, such that all components are orthogonal (perpendicular) to each other**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**PRINCIPAL COMPONENT TRANSFORMATION OF DATA: PC1 VS PC2**\n",
    "\n",
    "[Definitely try this out at setosa.io â€“ it's an extremely nice way to learn it!](http://setosa.io/ev/principal-component-analysis/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Image(filename='../assets/images/setosa_pc1.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why would we want to do PCA?\n",
    "\n",
    "---\n",
    "\n",
    "- We can reduce the number of dimensions (remove bottom number of components) and lose the least possible amount of variance information in our data.\n",
    "- Since we are assuming our variables are interrelated (at least in the sense that they together explain a dependent variable), the information of interest should exist along directions with largest variance.\n",
    "- The directions of largest variance should have the highest Signal to Noise ratio.\n",
    "- Correlated predictor variables (also referred to as \"redundancy\" of information) are combined into independent variables. Our predictors from PCA are guaranteed to be independent.\n",
    "\n",
    "---\n",
    "\n",
    "[Good paper on PCA](http://arxiv.org/pdf/1404.1100.pdf)\n",
    "\n",
    "[Nice site on performing PCA](http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html#pca-vs-lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Manual PCA Codealong\n",
    "\n",
    "---\n",
    "\n",
    "**MANUAL PCA STEPS:**\n",
    "\n",
    "1. Standardize data: centering is required, but full normalization is nice for visuals later.\n",
    "2. Calculate eigenvectors and eigenvalues from correlation or covariance matrix.\n",
    "3. Sort eigenvalues and choose eigenvectors that correspond to the largest eigenvalues. The number you choose is up to you, but we will take 2 for the sake of visualization here.\n",
    "4. Construct the projection weighting matrix $W$ from the eigenvectors.\n",
    "5. Transform the original dataset $X$ with $W$ to obtain the new 2-dimensional transformed matrix $Z$.\n",
    "\n",
    "---\n",
    "\n",
    "**DATA**\n",
    "\n",
    "We are going to be using a simple 75-row, 4-column dataset with demographic information. It contains:\n",
    "\n",
    "    age (limited to 20-65)\n",
    "    income\n",
    "    health (a rating on a scale of 1-100, where 100 is the best health)\n",
    "    stress (a rating on a scale of 1-100, where 100 is the most stressed)\n",
    "    \n",
    "All of the variables are continuous.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_style('white')\n",
    "\n",
    "demo = pd.read_csv('../assets/datasets/simple_demographics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Basic EDA\n",
    "\n",
    "Make a seaborn regplot for each of:\n",
    "\n",
    "1. age vs. income\n",
    "2. age vs. health\n",
    "3. age vs. stress\n",
    "\n",
    "Also make a pairplot of the entire dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# age vs income\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# age vs health\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# age vs stress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pairplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Subset and normalize\n",
    "\n",
    "Subset data to just:\n",
    "\n",
    "    income\n",
    "    health\n",
    "    stress\n",
    "\n",
    "We will be comparing the principal components to age specifically so we are leaving them out.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# subset to income, health, stress\n",
    "\n",
    "# normalize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Calculate correlation matrix\n",
    "\n",
    "We will be using the correlation matrix to calculate the eigenvectors and eigenvalues.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculate the eigenvalues and eigenvectors from the correlation matrix\n",
    "\n",
    "numpy has a convenient function to calculate this:\n",
    "\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(correlation_matrix)\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculate and plot the explained variance\n",
    "\n",
    "A useful measure is the **explained variance**, which is calculated from the eigenvalues. \n",
    "\n",
    "The explained variance tells us how much information (variance) is captured by each principal component.\n",
    "\n",
    "$$ ExpVar_i = \\bigg(\\frac{eigenvalue_i}{\\sum_j^n{eigenvalue_j}}\\bigg) * 100$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# component number vs cumulative variance explained\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Construct the Projection Matrix $W$\n",
    "\n",
    "This is simply a matrix of our top 2 eigenvectors.\n",
    "\n",
    "The eigenvectors are concatenated as columns.\n",
    "\n",
    "1. Start by ordering the eigenvectors by their corresponding eigenvalues biggest to smallest.\n",
    "- Concatenate the eigenvectors together. `np.hstack()` is useful for this.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pair eigenvalues and eigenvectors, then sort biggest to smallest by eigenvalue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create the projection matrix W\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Construct the Transformed 2D Matrix $Z$\n",
    "\n",
    "To do this, we take the dot product of our 3D demographic matrix $X$ with the projection matrix $W$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the 2D transformed dataset Z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Plot Principal Component 1 vs 2\n",
    "\n",
    "PC1 is the first column in $Z$, and PC2 is the second.\n",
    "\n",
    "Notice how they are un-correlated.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot data on PC1 vs PC2 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Plot age vs principal component 1 with regplot\n",
    "\n",
    "Look how tight the relationship is. PC1 took the shared variance out of income, health, and stress, which are intuitively directly related to increasing age. \n",
    "\n",
    "This principal component, or more specifically the column weighting matrix $W$, is essentially **capturing the latent age variance embedded in these variables.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot age vs PC1 using regplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Concatenate PC1 and PC2 to the full demographic (4D) dataset, then melt it with PC1 and PC2 and index variables\n",
    "\n",
    "1. Re-normalize so that all 4 variables are on the same scale.\n",
    "2. Remember the pandas melt code:\n",
    "\n",
    "```python\n",
    "melted_df = pd.melt(df, id_vars=['PC1','PC2'])\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize the full demo dataset (age, income, health, stress)\n",
    "\n",
    "# add PC1 and PC2 as new columns\n",
    "\n",
    "# melt the demo dataframe with PC1 and PC2 and id_vars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use lmplot to check out PC1 vs all 4 variables\n",
    "\n",
    "Make the `col` keyword argument \"variable\" and the `hue` keyword argument \"variable\" as well, assuming that's what you called them in the melt command (those are the defaults).\n",
    "\n",
    "Make `col_wrap = 2` and `size = 7` or something to make it nice.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot each variable against PC1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Use lmplot to do the same for PC2\n",
    "\n",
    "Notice how PC2 captures the variance of income, which was not captured well by PC1. This makes sense, as the variance each principal component captures has to be orthogonal to the other components.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot each variable against PC2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
