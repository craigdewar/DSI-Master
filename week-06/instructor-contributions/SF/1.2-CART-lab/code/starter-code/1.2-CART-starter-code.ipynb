{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees Lab\n",
    "\n",
    "In this lab we will discover how to apply decision trees to regression and classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Build a regression tree\n",
    "\n",
    "How do you build a decision tree? You're going to find out by building one in pairs.\n",
    "\n",
    "Your training data is a tiny dataset of [used vehicle sale prices](../../assets/datasets/used_cars.csv). Your goal is to predict Price for out-of-sample data. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Manually build the decision tree\n",
    "\n",
    "Here are your instructions:\n",
    "\n",
    "1. Read the data into Pandas.\n",
    "2. Explore the data by sorting, plotting, or split-apply-combine (aka `group_by`).\n",
    "3. Decide which feature is the most important predictor, and use that to make your first split. (Only binary splits are allowed!)\n",
    "4. After making your first split, you should actually split your data in Pandas into two parts, and then explore each part to figure out what other splits to make.\n",
    "5. Decide if you need additional splits along other features\n",
    "6. Stop making splits once you are convinced that it strikes a good balance between underfitting and overfitting. (As always, your goal is to build a model that generalizes well.)\n",
    "    - Note: You are allowed to split on the same variable multiple times!\n",
    "7. Draw your tree on a piece of paper or describe in in a markdown cell. Label your leaves with the mean Price for the observations in that \"bucket\".\n",
    "8. When you're finished, review your tree to make sure nothing is backwards. (Remember: follow the left branch if the rule is true, and follow the right branch if the rule is false.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 [Advanced alternative] ID3 algorithm pseudocode\n",
    "\n",
    "**You can alternatively code the recursive ID3 algorithm for decision trees.**\n",
    "\n",
    "The pseudocode is below, which you will convert into python code.\n",
    "\n",
    "---\n",
    "\n",
    "```\n",
    "ID3 (Examples, Target_Attribute, Candidate_Attributes)\n",
    "    Create a Root node for the tree\n",
    "    If all examples have the same value of the Target_Attribute,\n",
    "        Return the single-node tree Root with label = that value\n",
    "    If the list of Candidate_Attributes is empty,\n",
    "        Return the single node tree Root,\n",
    "            with label = most common value of Target_Attribute in the examples.\n",
    "    Otherwise Begin\n",
    "        A ← The Attribute that best classifies examples (most information gain)\n",
    "        Decision Tree attribute for Root = A.\n",
    "        For each possible value, v_i, of A,\n",
    "            Add a new tree branch below Root, corresponding to the test A = v_i.\n",
    "            Let Examples(v_i) be the subset of examples that have the value v_i for A\n",
    "            If Examples(v_i) is empty,\n",
    "                Below this new branch add a leaf node\n",
    "                    with label = most common target value in the examples\n",
    "            Else\n",
    "                Below this new branch add the subtree\n",
    "                    ID3 (Examples(v_i), Target_Attribute, Attributes – {A})\n",
    "    End\n",
    "    Return Root\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## How does a computer build a regression tree?\n",
    "\n",
    "The ideal approach would be for the computer to consider every possible partition of the feature space. However, this is computationally infeasible, so instead an approach is used called **recursive binary splitting:**\n",
    "\n",
    "- Begin at the top of the tree.\n",
    "- For every single predictor, examine every possible cutpoint, and choose the predictor and cutpoint such that the resulting tree has the **lowest possible mean squared error (MSE)**. Make that split.\n",
    "- Repeat the examination for the two resulting regions, and again make a single split (in one of the regions) to minimize the MSE.\n",
    "- Keep repeating this process until a stopping criteria is met.\n",
    "\n",
    "**How does it know when to stop?**\n",
    "\n",
    "1. We could define a stopping criterion, such as a **maximum depth** of the tree or the **minimum number of samples in the leaf**.\n",
    "2. We could grow the tree deep, and then \"prune\" it back using a method such as \"cost complexity pruning\" (aka \"weakest link pruning\").\n",
    "\n",
    "Method 2 involves setting a tuning parameter that penalizes the tree for having too many leaves. As the parameter is increased, branches automatically get pruned from the tree, resulting in smaller and smaller trees. The tuning parameter can be selected through cross-validation.\n",
    "\n",
    "Note: **Method 2 is not currently supported by scikit-learn**, and so we will use Method 1 instead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Build a regression tree in scikit-learn\n",
    "\n",
    "---\n",
    "\n",
    "### 2.a Use sklearn to construct the decision tree\n",
    "\n",
    "Building a tree by hand was not so easy, and also not ideal. Use scikit-learn to build an optimal regression tree. Do the following:\n",
    "\n",
    "1. Map the `type` column to a binary variable\n",
    "2. Create a matrix `X` that contains the feature values and a vector `y` that contains the price values\n",
    "3. Split the data into train-test using a random state of 42 and test_size of 30%\n",
    "4. Import and initialize the `DecisionTreeRegressor` class from scikit-learn\n",
    "5. Fit it to the training set\n",
    "6. Predict the values of the test set\n",
    "7. Display the predicted and actual values in a plot\n",
    "8. Use r2_score to judge the goodness of fit for the regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.b Examine effect of different parameters\n",
    "\n",
    "The `DecisionTreeRegressor` offers few global parameters that can be changed at initialization. For example one can set the `max_depth` or the `min_samples_leaf` parameters and impose global constraints on the space of solutions.\n",
    "\n",
    "1. Use `cross_val_score` with 3-fold cross validation to find the optimal value for the `max_depth` (explore values 1 - 10). \n",
    "    - Set `scoring='mean_squared_error'` as criterion for score. \n",
    "    - Set `random_state=1`\n",
    "2. Plot the error as a function of `max_depth` (max_depth on x axis, error on y axis)\n",
    "\n",
    "This code will get you the error:\n",
    "\n",
    "```python\n",
    "scores = cross_val_score(treereg, X, y, cv=3, scoring='mean_squared_error')\n",
    "current_error = np.mean(np.sqrt(-scores))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.c Feature importances\n",
    "\n",
    "The decision tree class exposes an attribute called `feature_importances_`.\n",
    "\n",
    "1. Check the importance of each feature. what's the most important feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.d Tree visualization\n",
    "\n",
    "Follow the example in the [documentation](http://scikit-learn.org/stable/modules/tree.html) to visualize the tree.\n",
    "\n",
    "You may have to install `pydot2` and `graphviz` if you don't have them already. (I can help with this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting a tree diagram\n",
    "\n",
    "How do we read this decision tree?\n",
    "\n",
    "**Internal nodes:**\n",
    "\n",
    "- `samples` is the number of observations in that node before splitting\n",
    "- `mse` is the mean squared error calculated by comparing the actual response values in that node against the mean response value in that node\n",
    "- First line is the condition used to split that node (go left if true, go right if false)\n",
    "\n",
    "**Leaves:**\n",
    "\n",
    "- `samples` is the number of observations in that node\n",
    "- `value` is the mean response value in that node\n",
    "- `mse` is the mean squared error calculated by comparing the actual response values in that node against \"value\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 3: Use GridSearchCV to find the best Regression Tree\n",
    "\n",
    "How do we know by pruning with max depth is the best model for us? Trees offer a variety of ways to pre-prune (that is, we tell a computer how to design the resulting tree with certain \"gotchas\").\n",
    "\n",
    "Measure           | What it does\n",
    "------------------|-------------\n",
    "max_depth         | How many nodes deep can the decision tree go?\n",
    "max_features      | Is there a cut off to the number of features to use?\n",
    "max_leaf_nodes    | How many leaves can be generated per node?\n",
    "min_samples_leaf  | How many samples need to be included at a leaf, at a minimum?  \n",
    "min_samples_split | How many samples need to be included at a node, at a minimum?\n",
    "\n",
    "1. Initialize reasonable ranges for all parameters and find the optimal combination using Grid Search.\n",
    "\n",
    "**To calculate best score, you need to use code:**\n",
    "\n",
    "```python\n",
    "print np.sqrt(-clf.best_score_)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Classification trees\n",
    "\n",
    "Classification trees are very similar to regression trees. Here is a quick comparison:\n",
    "\n",
    "|regression trees|classification trees|\n",
    "|---|---|\n",
    "|predict a continuous response|predict a categorical response|\n",
    "|predict using mean response of each leaf|predict using most commonly occuring class of each leaf|\n",
    "|splits are chosen to minimize MSE|splits are chosen to minimize a different criterion (discussed below)|\n",
    "\n",
    "Note that classification trees easily handle **more than two response classes**! (How have other classification models we've seen handled this scenario?)\n",
    "\n",
    "Here's an **example of a classification tree**, which predicts whether or not a patient who presented with chest pain has heart disease:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4.a Build a classification tree in scikit-learn\n",
    "\n",
    "We'll build a classification tree using the [Car Dataset](./assets/datasets/cars.csv).\n",
    "\n",
    "1. Load the dataset in pandas\n",
    "2. Check for missing values\n",
    "3. Encode all the categorical features to booleans using `pd.get_dummies`\n",
    "4. Encode the labels using LabelEncoder\n",
    "5. Split X and y with train_test split like above\n",
    "        train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "6. Fit a classification tree with `max_depth=3` on all data\n",
    "7. Visualize the tree using graphviz\n",
    "8. Compute the feature importances\n",
    "9. Compute and display the confusion matrix (use sklearn function)\n",
    "10. Release the constraint of `max_depth=3` by setting `max_depth=None` and see if the classification improves using cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "\n",
    "## [BONUS] 5. Classification tree visualization\n",
    "\n",
    "Visualize the last tree you build that had `max_depth=None`. Can you make sense of it? What does this teach you about decision tree interpretability?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
