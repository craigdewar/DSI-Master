{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Missing Political Data - A Review of Previous Methods Applied to something useful - Or clever ways to use machine learning techniques for pre/post processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Today's lab will focus on handling missing data, something we haven't had a ton of practice with. We will review some of the previous techniques covered in the immersive course thus far, but utilize it to something that we probably haven't really discussed much of: how to handle missing data -in depth. \n",
    "\n",
    "In general this topic is probably way more on the \"art\" in the science/art spectrum in the world of data science, but there are some well-established ways to deal with and impute missing data, depending on what you want to accomplish in the end (increase the power, remove NaNs, impute with a numerical/label to prevent errors from your ML algorithms etc.). \n",
    "\t\n",
    "Our overall goal today will be to show there is some kind of \"functional relationship\" between the \"missingness\" of the data, and features found in our data. By doing this, we can categorize the kind of \"missingness\" we are dealing with for a particular data-set.\n",
    "\n",
    "We'll briefly cover the 3 types of \"missingness\" and go straight to coding from there.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Types of \"Missingness\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Types of \"Missingness\"\n",
    "| Type  | Description  | \n",
    "|---|---|\n",
    " | Missing Completely at Random  | This is basically the best scenario, all NaN, NA, or blanks are distributed totally \"at random\" can be safetly omitted  |\n",
    " | Missing at Random  | This is less strong, but is \"random\" given the sample you are using. This is what we're aiming at for our analysis, functionally, we want to show that our missing data isn't dependent on data we haven't observed or accounted for in our data set   | \n",
    " | Missing not at Random  | \"There is a data generating process that yields missing values\". Basically, it means there is some \"pattern\" to the 'missingness' |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing the Inclusion Indicator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As stated, the type of “missingness” we are most concerned about is the last row, \"Missing not at Random\". And as good Data Scientist, we understand what \"data generating process\" really means: We can make an equation of sorts to \"model\" the “missingness” in our data set. If we can convincingly show that this model accounts for \"most\" (again we're not being stringent statisticians so that word will be left up to you to define for the moment) of the observable variation, we can be (relatively) well-at-ease that our \"missingness\" isn't functionally related to some features we don't have control/accounted/recorded in our data.\n",
    "\n",
    "Before we move forward, we have to define the \"inclusion indicator\". We say I is an inclusion indicator if : $$\\begin{array}{cc}\n",
    "  I=\\{ & \n",
    "    \\begin{array}{cc}\n",
    "      1 & x: missing \\\\\n",
    "      0 & x: \\neg{missing} \\\\\n",
    "    \\end{array}\n",
    "\\end{array} $$\n",
    "\n",
    "Simple enough? Let's read on, and maybe we'll see how Bayesian analysis slips its slimy tenticles into seemingly simple things.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading up data with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to load up our familiar polling data from our previous labs. However we will take the analysis much broader this time, and we will be using a version of the data set where we have not removed missing values... because after all, that's the point of this entire lab! \n",
    "\n",
    "So load up the data and the libraries we will need to get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import math\n",
    "import pylab as py\n",
    "import sys\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "pre_poll = pd.read_csv('F:/Bayesian lab/polls.csv')\n",
    "pre_poll.head(10)\n",
    "del pre_poll['Unnamed: 0']\n",
    "pre_poll['org'].value_counts()\n",
    "del pre_poll['org']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1 - Construct the Inclusion indicator and append the column to the table "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to delete extraneous columns for our purposes: year, survey, female, black, and weight. Also build an 'inclusion' indicator column that will be 1 when bush is missing a value, and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del pre_poll['year']; del pre_poll['survey']; del pre_poll['female']; del pre_poll['black']; del pre_poll['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pre_poll['inclusion'] = np.where(pd.isnull(pre_poll['bush']), 1, 0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2 - Prepare your data by converting it into numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of our ML work will be better suited if the input data is contained in a numpy container (and as I'll be saying many times throughout the week - although Pandas is a great set of methods with awesome data containers/types, numpy is often the framework real hardcore machine learning is developed on, so it's best you start to get a taste of numpy as well as Pandas. Please go ahead and convert the data into that structure now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bush</th>\n",
       "      <th>state</th>\n",
       "      <th>edu</th>\n",
       "      <th>age</th>\n",
       "      <th>inclusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bush  state  edu  age  inclusion\n",
       "0   1.0      5    1    1          0\n",
       "1   1.0     30    3    2          0\n",
       "2   0.0     17    1    0          0\n",
       "3   1.0     28    2    1          0\n",
       "4   1.0     15    2    0          0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "encode = preprocessing.LabelEncoder()\n",
    "pre_poll['age'] = encode.fit_transform(pre_poll.age) \n",
    "pre_poll['state'] = encode.fit_transform(pre_poll.state)\n",
    "pre_poll['edu'] = encode.fit_transform(pre_poll.edu)\n",
    "\n",
    "\n",
    "pre_poll.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 3 - Split the data  70/30 train/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data in the ordinary way, making sure you have a 70/30 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_poll['train'] = np.random.uniform(0, 1, len(pre_poll)) <= .70\n",
    "pre_poll_train = pre_poll[pre_poll['train'] == True]\n",
    "test = pre_poll[pre_poll['train'] == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wait... how the hell can we tell if something is \"Missing not at random\"? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a real good question, and surprisingly, there isn't much clarity on this front. Although recall, above that I mentioned that one way is to understand \"how much\" of the variation in your data your model is accounting for. We'll do some preliminary work on that front here, but I'm going to ask you to think deeper ask yourself: \n",
    "\n",
    "1. How can I apply what I've learned in Regressions to this problem? \n",
    "2. What are other metrics I could utilize to account for variation in data outside of regressions? \n",
    "\n",
    "So one possible approach we've strongly pointed towards is to construct regression models with the inclusion indicator as a target, and see what sort of performance you can get out of those family of techniques. \n",
    "\n",
    "Let's try to throw a bunch of things at this problem and see what makes sense (or see what we get).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pymc as pm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Logistic Regression to model the \"missingness\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4 - Build a classical logistic regression to model the inclusion indicator as a target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.413777\n",
      "         Iterations 6\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:              inclusion   No. Observations:                 9433\n",
      "Model:                          Logit   Df Residuals:                     9430\n",
      "Method:                           MLE   Df Model:                            2\n",
      "Date:                Sat, 21 May 2016   Pseudo R-squ.:                0.002296\n",
      "Time:                        00:03:47   Log-Likelihood:                -3903.2\n",
      "converged:                       True   LL-Null:                       -3912.1\n",
      "                                        LLR p-value:                 0.0001257\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "edu           -0.6066      0.024    -25.112      0.000        -0.654    -0.559\n",
      "state         -0.0191      0.002    -11.474      0.000        -0.022    -0.016\n",
      "age           -0.2629      0.024    -10.860      0.000        -0.310    -0.215\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# This is my favorite logistiic implementation, very simple, close to R's lm. \n",
    "import statsmodels.formula.api as sm\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "rhs_columns = ['edu', 'state', 'age']\n",
    "inclusion_lm = sm.Logit(pre_poll_train['inclusion'], pre_poll_train[rhs_columns]).fit()\n",
    "\n",
    "inclusion_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 5 - Build a vector of prediction from the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05117677,  0.18237426,  0.06794271, ...,  0.28354629,\n",
       "        0.28354629,  0.14228471])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = inclusion_lm.predict(test[rhs_columns]); y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using K-Nearest Neighbor for imputing missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 6 - Build a K-NN model (k = 5), to model the inclusion indicator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point of this model isn't really to shed more light on the \"missingness\", but rather to actually impute values into our column of data that contains missing values. Still, it's a good exercise to go through. After you've done the imputation, take a random subset of these imputed values and think about the results, is doing this a good way to fill in values? Would it be easier to do something simpler i.e. take the average for numerical data, or just select some label as fill-in for categorical data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_impute = KNeighborsClassifier(n_neighbors = 5)\n",
    "knn_impute.fit(pre_poll_train[rhs_columns], pre_poll_train['inclusion'])\n",
    "\n",
    "knn_pred = knn_impute.predict(test[rhs_columns])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing with Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 7 - Build a Random forest to model the inclusion indicator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the KNN, this is more about actually doing the imputation. However still a good review/exercise, compare your results with the KNN. How can we objectively measure relative performance? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "\n",
    "random_for = RandomForestClassifier(n_estimators=1000)\n",
    "random_for.fit(pre_poll_train[rhs_columns], pre_poll_train['inclusion'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Brief Aside - Reviewing Bayesian Logistic Regression and using it for imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To the instructor. This decision was made as I looked through the curriculum and saw that PyMC was the better choice cause of it's attachment to several key projects, including Theano. I feel that PyMc is also much more 'natural' as a library and is just easier to use. I recommended to Jeff we make the change and he agreed. You may want to look over the code and dedicate some time to going through basic method calls for PyMC. Since they have done this in PyStan, and there are very clear equivalences between the two, they should be able to pick this up much faster than just seeing it without any context the first time around. You may want to also go over the difference between deterministic and stochastic function specifications\n",
    "\n",
    "We are going to step away from PyStan and try our hand with PyMC for the rest of the Bayesian section. PyStan is an important tool, and is useful since it also has cross-platform integration with R. However, for purposes of ease-of-use, power, and expandability, PyMC, the latest version of which is integrated with the Theano Deep-Learning system, is a better choice. It's also syntactically closer to what you'd be use to in scikit-learn and other Python/R method-calls. \n",
    "\n",
    "Recall that we need to define prior distributions in the Bayesian frameworks, so please go ahead and figure out how many coefficients/parameters you will have, and proceed to define the weights. A 'uniform' or 'uninformative' prior is appropriate for this walk-through purpose.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A little primer into PyMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you should be getting comfy with reading docs by now. However, PyMC is one of the best written docs I've come across. Strangely enough it straddles the line between coding/programming doc and basic statistical exposition. Please check out the following: \n",
    "\n",
    "https://pymc-devs.github.io/pymc/distributions.html , this is the section on distributions. \n",
    "\n",
    "For our purposes, we're going to be using only a few key methods from PyMC. This includes the distribution methods, examples given below. Please study it and note where there are some similarities with PyStan. I've annotated the example to help out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import relevant modules   - Example provided from PyMC docs\n",
    "import pymc \n",
    "import numpy as np\n",
    "\n",
    "# Some data      - This is just simulated data, in real life, you won't have this section as a function, but maybe a pd/numpy \n",
    "#                  data frame/array etc.\n",
    "n = 5*np.ones(4,dtype=int)\n",
    "x = np.array([-.86,-.3,-.05,.73])\n",
    "\n",
    "# Priors on unknown parameters      - Note - Similar to PyStan, because it's a Bayesian analysis, we have to define the distribution\n",
    "#                                     for all of our parameters. Notice there's something quotes 'alpha' that is where you put\n",
    "#                                     your parameter's label. You can put anything as a label, as long as it's a string. The reason\n",
    "#                                     we have a label will become clear as we work more with PyMC. Just keep in mind of it for now.\n",
    "alpha = pymc.Normal('alpha',mu=0,tau=.01)\n",
    "beta = pymc.Normal('beta',mu=0,tau=.01)\n",
    "\n",
    "# Arbitrary deterministic function of parameters             - Again, defining our model's form. \n",
    "@pymc.deterministic                                          # Just put this string up for now whenever you specify your model \n",
    "def theta(a=alpha, b=beta):                                  # We'll discuss later\n",
    "    \"\"\"theta = logit^{-1}(a+b)\"\"\"\n",
    "    return pymc.invlogit(a+b*x)\n",
    "\n",
    "# Binomial likelihood for data\n",
    "d = pymc.Binomial('d', n=n, p=theta, value=np.array([0.,1.,3.,5.]),\\\n",
    "                  observed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great let's start specifying our first PyMC model. We're going to define a model with 3 parameters, and 1 constant, so we'll need to specify 4 distributions, say w0 (constant), w1, w2, w3. \n",
    "\n",
    "For simplicity, let's just have all of these parameters be normal in the following form: \n",
    "\n",
    "**my_parameter = pm.Normal('my_parameter', mu = 0, tau = 0.000001, value = 0)**\n",
    "\n",
    "Again mu is just your mean ($\\mu$), and tau is just your standard deviation/scaling term ($\\tau$). So we would write a standard normal as pm.Normal('standard normal', mu = 0.0, tau = 1.0, value = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 8 - Start building your Bayesian Logistic Regression by defining your coeffecient priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pymc as pm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "w0 = pm.Normal('w0', 0, 0.000001, value = 0)    \n",
    "w1 = pm.Normal('w1', 0, 0.000001, value = 0)   \n",
    "w2 = pm.Normal('w2', 0, 0.000001, value = 0)   \n",
    "w3 = pm.Normal('w3', 0, 0.000001, value = 0)   \n",
    "w0.value; w1.value; w2.value; w3.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 9 - Build your bayesian logistic regression form and run it through a Bernoulli method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify a simple univariate logistic regression, just pick one of your parameters and the constant. **Hint:** Think the Sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymc.distributions.Bernoulli 'observed' at 0x0000000015613C18>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@pm.deterministic\n",
    "def bayesian_logistic(w0 = w0, w1 = w1, w2 = w2, w3 = w3, x1 = pre_poll_train['age'], x2 = pre_poll_train['state'], x3 = pre_poll_train['edu']):  #technically abusing the use of default\n",
    "    return 1.0 / (1.0 + np.exp(w0 + w1*x1))\n",
    "\n",
    "observed = pm.Bernoulli('observed', bayesian_logistic, value = pre_poll_train['inclusion'], observed=True); observed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 10 - Go ahead and push through your Bayesian model through the computation segment by calling the Monte-Carlo Markov Chain procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to PyStan, you will now have to push your model through the MCMC procedure. Since this is the first time you're seeing it, I've included the code here. Also, if you want to read up on it more checkout : https://pymc-devs.github.io/pymc/modelfitting.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-----------------100%-----------------] 100000 of 100000 complete in 168.7 sec"
     ]
    }
   ],
   "source": [
    "m = pm.MCMC(locals())\n",
    "m.sample(100000, 50000, 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the trace graph. Remember to use plt.show() at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting w2\n",
      "Plotting w1\n",
      "Plotting w0\n",
      "Plotting w3\n"
     ]
    }
   ],
   "source": [
    "pm.Matplot.plot(m)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing some basic comparisons of results and forecasting efficaciousness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we need to compare our results, construct ROC scores for each of the 3 methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the AUC for your non-Bayesian Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the ROC curve : 0.601953\n"
     ]
    }
   ],
   "source": [
    "fpr, tpr, thresholds =roc_curve(test['inclusion'], y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"Area under the ROC curve : %f\" % roc_auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the AUC for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the ROC curve : 0.662932\n"
     ]
    }
   ],
   "source": [
    "random_pred = random_for.predict_proba(test[rhs_columns]); random_pred\n",
    "\n",
    "fpr, tpr, _ = roc_curve(test['inclusion'], random_pred[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"Area under the ROC curve : %f\" % roc_auc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the AUC for KNN Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the ROC curve : 0.526792\n"
     ]
    }
   ],
   "source": [
    "fpr, tpr, thresholds =roc_curve(test['inclusion'], knn_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"Area under the ROC curve : %f\" % roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From my understanding determining the type of missingness is an art, so I feel the students should think about how they individually would view this exercise as a \"first pass\" on the \"missingness\" of the data, and how it will serve their future analysis (weighting observations, doing some kind of correction to account for \"missingess\", or nothing at all etc.)\n",
    "\n",
    "**Open Ended Questions** So can we be fairly confident that there is some kind of functional relationship between the indicator variable and the few columns we studied in our data set? Is it obvious that there are probably other factors impacting \"missingness\" from this data? Which type of \"missingness\" are we probably in, and what does that say about the state of our missing data and how we should approach modeling on this data set in the future? What further actions can we take to augment this analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Bootstrap to deal with small samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we're going do a simple bootstrap estimation of a regression equation using the weighted least squares estimation procedure. \n",
    "\n",
    "Reference for weighted least squares in docs: \n",
    "http://statsmodels.sourceforge.net/devel/generated/statsmodels.regression.linear_model.WLS.html\n",
    "\n",
    "Reference for the gamma method in numpy: \n",
    "\n",
    "http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.random.gamma.html\n",
    "\n",
    "\n",
    "We're goign first define an empty numpy array with 1000 rows and 4 columns. Remember, both in Python and R, building an empty container/structure before populating it with values is often more effecient with respect to run-time vis-a-vis iteratively adding rows/columns. Again, as we mature as data scientist, we will eventually need to think about these issues (especially as we build more complicated machine learning systems). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Define our feature set \n",
    "\n",
    "test_col_1 = pre_poll_train[rhs_columns]\n",
    "\n",
    "# Add a constant term for our regression equation\n",
    "\n",
    "test_col_1 = sm.add_constant(test_col_1) \n",
    "         \n",
    "    \n",
    "y = pre_poll_train['inclusion']; N = len(y); M = np.empty((1000,4))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the Bayesian Bootstrap procedure. Peruse this paper if you'd like to understand the fundamental background for the procedure: https://projecteuclid.org/download/pdf_1/euclid.aos/1176345338. Unfortunately, I couldn't find more clearer exposition on this, but I'll cut to the chase with respect to the differences between \"ordinary bootstrap\" and \"bayesian bootstrap\", basically, we are drawing from a dirchilet distribution for the bayesian variant. \n",
    "\n",
    "So our aim is as follows: \n",
    "\n",
    "1. We want to create a vector of dirchilet draws\n",
    "2. Use those draws as weights to pair with our features in a weighted least-squares procedure\n",
    "3. We wish to to create a basic statistical summarization of the results (hint- look up the describe method for a pandas data-frame : double hint- it's ok to store the results of the WLS procedure to a pandas data frame). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write a for lop that creates a vector of length 1000, each being populated by a Dirchilet draw\n",
    "\n",
    "for b in range (0,1000):\n",
    "    W = np.random.gamma(1.,1.,N) \n",
    "    W = W/np.sum(W)              \n",
    "    \n",
    "    result = sm.WLS(y, test_col_1 ,weights=W).fit()\n",
    "    M[b,:] = np.matrix(result.params) \n",
    "    M[:,0] = M[:,1]/M[:,2]          \n",
    "                                \n",
    "\n",
    "# Put Bayesian Bootstrap results in a pandas dataframe        \n",
    "BB = pd.DataFrame({'constant':M[:,0], 'edu':M[:,1], 'state':M[:,2], 'age':M[:,3]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BB.quantile(q=[0.025, 0.05, 0.5, 0.95, 0.975])\n",
    "\n",
    "BB.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
