{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking the Hypothesis Testing 1 Step Forward with a Bayesian Twist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab will be divided into two sections. One we'll manually compute some of the Bayesian statistical testing procedures manually/step-wise using Python as a calculator, and the second half, we'll go ahead and use PyMC to go through the analysis and leverage the libraries to provide more automatic results for us. \n",
    "\n",
    "The first thing we'll do is load up the data set, which contains 3 columns, the number of clicks for a website on a given day, the number of sales conversions (actual sales) on a given day, and whether it was the weekend.\n",
    "\n",
    ">Note to instructor: This data is simulated, and I generated most of the click data using a Poisson process. Part of the \"extra\" challenge in this lab will be to see if the fast-learners can figure out which distribution to use outside of the normal distribution I've outlined here for them to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load the data\n",
    "\n",
    "CTR_dat = pd.read_csv('F:/test/CTR_Sim.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clicks Conv</th>\n",
       "      <th>Clicks</th>\n",
       "      <th>Weekend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Clicks Conv  Clicks  Weekend\n",
       "0           11      19        0\n",
       "1           10      20        0\n",
       "2           11      17        0\n",
       "3           15      14        0\n",
       "4           11      18        0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CTR_dat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our question will be simple: If we calculate the mean clicks for both the weekend and non-weekend days, can we determine which mean represents the \"true\" mean for the site? i.e. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we wanted to know which set of clicks (weekend or weekday) represented the \"true\" set of clicks on the website, how would we state it formally in a statistical test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: $H_0$: Weekend Mean, $H_1$: Weekday Mean "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the Weekend Mean and the Weekday Mean of clicks and assign them to the variables Weekend_Mean and Weekday_Mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.0842105263\n",
      "19.8335664336\n"
     ]
    }
   ],
   "source": [
    "Weekend_Mean = CTR_dat[CTR_dat['Weekend']==1]['Clicks'].mean()\n",
    "Weekday_Mean = CTR_dat[CTR_dat['Weekend']==0]['Clicks'].mean()\n",
    "\n",
    "print Weekend_Mean\n",
    "print Weekday_Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't really have any beliefs one way or the other. How would I state this in Bayesian terms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: Let the Prior P($H_0$) = $\\frac{1}{2}$ and P($H_1$) = $\\frac{1}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have computed our priors, we need to move onto our likelihood function. Suppose I know that the true variance of clicks is 8. Further, we just got more 100 more days of observations all from mobile (as opposed to computer accessed) and the mean clicks is 25. Compute the Standard Error (SER) - Assuming the number of clicks is being drawn from a normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: $\\sqrt{\\frac{25}{100}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the Z-scores and the associated Likelihoods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: $z_0 = \\frac{25-20.084}{.5}$=9.832 and $z_1 = \\frac{25-19.8335}{.5}$=10.333 therefore, $l_0 = \\frac{e^\\frac{-z_0}{2}}{\\sqrt{2\\pi}}$. Therefore, the likelihoods are equal to $\\frac{.00292}{2}$ and $\\frac{.00227}{2}$ respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the total probability (remember from the first lecture):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: P(X)=$\\frac{1}{4}.00292+\\frac{1}{4}.00227$=.0012975"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's bring it all together, what is the posterior for the first hypothesis and the second hypothesis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** $P(H_0|X)=\\frac{\\frac{1}{2}(.00146)}{.0012975}$=.5626 and $P(H_1|X)=\\frac{\\frac{1}{2}(.001135)}{.0012975}$=.43737"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've just completed a hands-on exercise computing a statistical test using Bayesian analysis. But how do we interpret it? Think about and discuss with your colleagues a while. Is this strong evidence for one hypothesis or another? What are the possible ways we could \"measure\" the strength of $H_0$ vis-a-vis $H_1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: One simple way would be to take the quotient between the two probabilities and declare a certain threshold (say 2,3, 4) by which if the quotient is above, we declare there being \"strong\", \"medium\" or \"low\" evidence. How do you interpret the quotient between our two hypotheses above? Is it strong? Weak? Evidence? What does it mean if the quotient was close to 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credible Interval vs Confidence Interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To review again in plain English, there is a clear difference between Credible and Confidence interval interpretations. \n",
    "\n",
    "**Confidence Intervals** : As the number of experimental trials increases X.X% ('X.X' being the desired confidence 'level') of the time, the confidence intervals will contain the \"true\" value of the parameter you are interested in (usually the mean). \n",
    "\n",
    "**Credible Intervals** : Given some data, there is a X.X% probability that the \"true\" value of your parameter is contained within your credible interval. \n",
    "\n",
    "As we discussed, the credible interval is much easier to explain to normal people, and does not require you to go into horse-shoe metaphors, or any other linguistic legerdemain. \n",
    "\n",
    "A subtle point that should be emphasized, much like other parts of Bayesian analysis, when one does the Bayesian \"inversion\" trick, often times what's being fixed in an analysis and what's being varied switches. No different here, observe that in the confidence interval, the parameter is fixed, but each iteration brings a new confidence interval (hence the horse-shoe metaphor). \n",
    "\n",
    "However, in the credible interval, it's the opposite, the interval is what's fixed, but the parameter value is what changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a Bayesian basic stats test process with scikit-learn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, we proceed with putting the above finger-calculations into code, let's do some quick visualizations comparing the data to a fitted normal distribution. In the following below, create 2 plots with a matplotlib graph that shows a histogram of that overplayed with distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = CTR_dat['Clicks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "post = st.norm.pdf(x = q, loc = CTR_dat['Clicks'][CTR_dat['Weekend']==0].mean(), scale = CTR_dat['Clicks'][CTR_dat['Weekend']==0].std()); \n",
    "plt.plot(q, post, '-o')\n",
    "plt.hist(q,normed = True) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "post = st.norm.pdf(x = q, loc = CTR_dat['Clicks'][CTR_dat['Weekend']==1].mean(), scale = CTR_dat['Clicks'][CTR_dat['Weekend']==1].std()); \n",
    "plt.plot(q, post, '-o')\n",
    "plt.hist(q, normed = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot both distributions over each other. Does the graph visually confirm what you suspected based on the finger exercises above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "post1 = st.norm.pdf(x = q, loc = CTR_dat['Clicks'][CTR_dat['Weekend']==1].mean(), scale = CTR_dat['Clicks'][CTR_dat['Weekend']==1].std()); \n",
    "post2 = st.norm.pdf(x = q, loc = CTR_dat['Clicks'][CTR_dat['Weekend']==0].mean(), scale = CTR_dat['Clicks'][CTR_dat['Weekend']==0].std()); \n",
    "plt.plot(q, post2, label = '1')\n",
    "plt.plot(q, post1, label = '2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Although scikit-learn isn't explicitly a Bayesian computing library, we are going to use standard statistical methods from it to compute the two posteriors above.\n",
    "\n",
    "First browse the following page (scikit-learn stats docs) http://docs.scipy.org/doc/scipy-0.14.0/reference/stats.html\n",
    "\n",
    "Now using the stats suite of methods, define the normal likelihood for both hypothesis 1 and 2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0097504043246359629"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normal_likelihood for hypothesis 1\n",
    "\n",
    "n = 100\n",
    "z1 = 9.832\n",
    "norm_like_1 = st.norm(loc = 20.084, scale = 5).pdf(z1); norm_like_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013123162954935328"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normal likelihood for hypothesis 2\n",
    "\n",
    "z2 = 10.333\n",
    "norm_like_2 = st.norm(loc = 19.833, scale = 5).pdf(z2); norm_like_2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Great, proceed forward and define the total probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011436783639785645"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = .5\n",
    "\n",
    "tot_prob = weight*norm_like_1 + (1-weight)*norm_like_2; tot_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now bring it all together and define both posterior distributions based on the work above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42627388222667778"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Posterior 1\n",
    "\n",
    "posterior1 = .5*norm_like_1/tot_prob; posterior1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57372611777332216"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Posterior 2\n",
    "\n",
    "posterior2 = .5*norm_like_2/tot_prob; posterior2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge Problem 1: Building a more automatic Posterior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that in the early lectures this week, we discussed gamma that is used to scale the numerator of the posterior distribution to ensure it's a proper probability (all values between 0 and 1). Notice in our above implementation, we used the total probability. Can you write a python function that will compute the posterior, and include gamma? Recall, that gamma requires you to select values for some 'hyper-parameters' (usually denoted a, b respectively). Your function will have to take these along with other values as its input vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def posterior_compute(x,y,z):\n",
    "    # Put code here\n",
    "    return post_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge Problem 2 : Wait- is there a better distribution? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you do one better? I assumed that the clicks were univariate normal... but is that true? Think intuitively and do some simple data exploration, could there be another distribution that better fits the click data? Do some work below, work with groups of 2 and see if you can modify the above to get a tighter fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional applied example - testing the functional relationship of click conversion and clicks for weekday and weekends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To close out today's lab, we're going to do an additional exercise building a simple linear regression model with PyMC. This time however, you will look to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pymc\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "beta0 = pymc.Uniform('alpha', -100, 100)\n",
    "\n",
    "@pymc.stochastic(observed = False)\n",
    "def beta1(value = 0):\n",
    "    return -1.5*np.log(1+value**2)\n",
    "\n",
    "@pymc.stochastic(observed = False)\n",
    "def sigma(value = 1):\n",
    "    return -np.log(abs(value))\n",
    "\n",
    "@pymc.deterministic\n",
    "def y_model(x=CTR_dat['Clicks'], alpha=beta0, beta=beta1):\n",
    "    return beta0 + beta1*x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the model above into a PyMC model instance. Remember, to use the precision (and not the variance) for tau - Review first few lessons for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-----------------100%-----------------] 10000 of 10000 complete in 5.4 sec"
     ]
    }
   ],
   "source": [
    "y = pymc.Normal('y', mu = y_model, tau = 1.0/sigma**2, observed = True, value = CTR_dat['Clicks Conv'])\n",
    "\n",
    "reg_mod = dict(alpha=alpha, beta = beta, sigma = sigma, y_model = y_model, y = y)\n",
    "S = pymc.MCMC(reg_mod)\n",
    "S.sample(10000, 5000, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the credible interval for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pylab import hist, show\n",
    "\n",
    "\n",
    "pymc_trace = [S.trace('alpha')[:], S.trace('beta')[:], S.trace('sigma')[:]]\n",
    "hist(S.trace('beta')[:])\n",
    "show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute trace plots for all parameters in the model. How would you diagnose it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting sigma\n",
      "Plotting alpha\n",
      "Plotting beta\n"
     ]
    }
   ],
   "source": [
    "from pymc.Matplot import plot\n",
    "plot(S)\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
